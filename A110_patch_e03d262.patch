From e03d26212fd4345fdad7514f4d1eddeac5b2a2f4 Mon Sep 17 00:00:00 2001
From: Gustavo Schneiter <gmhschn@gmail.com>
Date: Fri, 19 Sep 2025 22:47:25 -0300
Subject: [PATCH] A110: invariants gate (CI, scripts, docs)

---
 .config/nextest.toml                  |   5 +
 .github/workflows/a110-invariants.yml | 113 ++++++++
 docs/a110-branch-protection.md        |  45 +++
 docs/rollback-a110.md                 | 102 +++++++
 scripts/a110_add_badge.sh             |  88 ++++++
 scripts/a110_gate.py                  | 386 ++++++++++++++++++++++++++
 scripts/a110_run_invariants.sh        | 306 ++++++++++++++++++++
 7 files changed, 1045 insertions(+)
 create mode 100644 .config/nextest.toml
 create mode 100644 .github/workflows/a110-invariants.yml
 create mode 100644 docs/a110-branch-protection.md
 create mode 100644 docs/rollback-a110.md
 create mode 100644 scripts/a110_add_badge.sh
 create mode 100644 scripts/a110_gate.py
 create mode 100644 scripts/a110_run_invariants.sh

diff --git a/.config/nextest.toml b/.config/nextest.toml
new file mode 100644
index 0000000..6a109e1
--- /dev/null
+++ b/.config/nextest.toml
@@ -0,0 +1,5 @@
+[profile.ci]
+fail-fast = false
+
+[profile.ci.junit]
+path = "junit.xml"
diff --git a/.github/workflows/a110-invariants.yml b/.github/workflows/a110-invariants.yml
new file mode 100644
index 0000000..261d5ce
--- /dev/null
+++ b/.github/workflows/a110-invariants.yml
@@ -0,0 +1,113 @@
+name: A110 — Invariants Gate
+
+on:
+  pull_request:
+    branches:
+      - main
+  workflow_dispatch:
+
+concurrency:
+  group: ${{ github.workflow }}-${{ github.event.pull_request.number || github.ref }}
+  cancel-in-progress: true
+
+permissions:
+  contents: read
+  checks: write
+  pull-requests: write
+
+jobs:
+  run-gate:
+    name: Run invariants gate
+    runs-on: ubuntu-latest
+    steps:
+      - name: Checkout repository
+        uses: actions/checkout@v4
+
+      - name: Install Rust toolchain
+        uses: dtolnay/rust-toolchain@stable
+
+      - name: Cache cargo directories
+        uses: actions/cache@v4
+        with:
+          path: |
+            ~/.cargo/registry
+            ~/.cargo/git
+            ~/.rustup
+            target
+          key: ${{ runner.os }}-cargo-${{ hashFiles('**/Cargo.lock') }}
+          restore-keys: |
+            ${{ runner.os }}-cargo-
+
+      - name: Install nextest
+        uses: taiki-e/install-action@v2
+        with:
+          tool: nextest
+
+      - name: Run invariant suite
+        run: |
+          bash scripts/a110_run_invariants.sh
+        env:
+          NEXTEST_PROFILE: ci
+
+      - name: Execute invariants gate
+        run: |
+          python3 scripts/a110_gate.py
+
+      - name: Upload gate artifacts
+        uses: actions/upload-artifact@v4
+        with:
+          name: a110-invariants-artifacts
+          path: artifacts
+
+      - name: Comment on pull request
+        if: github.event_name == 'pull_request'
+        uses: actions/github-script@v7
+        with:
+          github-token: ${{ secrets.GITHUB_TOKEN }}
+          script: |
+            const core = require('@actions/core');
+            const {context} = require('@actions/github');
+            const fs = require('fs');
+            const path = 'artifacts/a110-summary.md';
+            if (!fs.existsSync(path)) {
+              core.info(`Summary file not found at ${path}; skipping comment.`);
+              return;
+            }
+            const body = fs.readFileSync(path, 'utf8');
+            if (!body.trim()) {
+              core.info('Summary file is empty; skipping comment.');
+              return;
+            }
+            await github.rest.issues.createComment({
+              owner: context.repo.owner,
+              repo: context.repo.repo,
+              issue_number: context.issue.number,
+              body,
+            });
+
+      - name: Evaluate gate status
+        uses: actions/github-script@v7
+        with:
+          script: |
+            const core = require('@actions/core');
+            const fs = require('fs');
+            const path = 'artifacts/a110-summary.json';
+            if (!fs.existsSync(path)) {
+              core.setFailed(`Summary JSON not found at ${path}.`);
+              return;
+            }
+            let data;
+            try {
+              data = JSON.parse(fs.readFileSync(path, 'utf8'));
+            } catch (error) {
+              core.setFailed(`Failed to parse ${path}: ${error.message}`);
+              return;
+            }
+            const gateStatus = data.gate_status;
+            const shouldFail = gateStatus === 'fail' || (gateStatus === undefined && data.gate_should_fail === true);
+            if (shouldFail) {
+              core.setFailed('Gate reported failure status.');
+              return;
+            }
+            const statusMessage = gateStatus !== undefined ? gateStatus : (data.gate_should_fail ? 'fail' : 'pass');
+            core.info(`Gate status: ${statusMessage}`);
\ No newline at end of file
diff --git a/docs/a110-branch-protection.md b/docs/a110-branch-protection.md
new file mode 100644
index 0000000..4f73f5c
--- /dev/null
+++ b/docs/a110-branch-protection.md
@@ -0,0 +1,45 @@
+# A110 — Branch Protection (Required Check)
+
+## Pré-requisitos
+
+Permissão de administrador no repositório e acesso à organização; o workflow deve estar nomeado exatamente como **A110 — Invariants Gate**.
+
+## Via GitHub UI
+
+1. Acesse **Settings → Branches → Branch protection rules** e clique em **Add rule** (ou edite a regra existente para `main`).
+2. Defina **Branch name pattern** como `main`.
+3. Em **Require status checks to pass before merging**, selecione **A110 — Invariants Gate**.
+4. (Opcional) Marque **Require branches to be up to date before merging** para exigir merge com o estado mais recente de `main`.
+5. Clique em **Save changes** para aplicar a proteção.
+
+## Via GitHub CLI (gh api)
+
+Execute um PUT em `PUT /repos/:owner/:repo/branches/main/protection`, fornecendo um payload JSON semelhante ao exemplo abaixo:
+
+```json
+{
+  "required_status_checks": {
+    "strict": true,
+    "contexts": ["A110 — Invariants Gate"]
+  }
+}
+```
+
+* `strict` é opcional; defina como `true` apenas se quiser exigir que o branch esteja atualizado com `main` antes do merge.
+* É necessário possuir escopos `repo` e `admin:repo_hook` no token utilizado pelo `gh`.
+* Organizações podem ter políticas que sobreponham configurações locais de proteção.
+
+## Validação
+
+1. Abra um pull request de teste contra `main`.
+2. Confirme que o botão **Merge** permanece bloqueado enquanto o workflow **A110 — Invariants Gate** não estiver concluído com sucesso.
+3. Para demonstrar o bloqueio, force um teste sintético [P2] a falhar; o PR deve ficar vermelho e impossibilitado de merge até que o check fique verde.
+
+## Reverter a regra
+
+* **UI**: desmarque o status check **A110 — Invariants Gate** na regra de proteção do branch `main` e salve.
+* **CLI**: envie um payload para o mesmo endpoint removendo o contexto `"A110 — Invariants Gate"` de `required_status_checks.contexts`.
+
+## Observações
+
+A proteção convive com outras regras (por exemplo, `CODEOWNERS`, aprovações de review e conversas obrigatórias). Priorize o conjunto mínimo de proteções para evitar deadlocks entre exigências conflitantes.
\ No newline at end of file
diff --git a/docs/rollback-a110.md b/docs/rollback-a110.md
new file mode 100644
index 0000000..884a16c
--- /dev/null
+++ b/docs/rollback-a110.md
@@ -0,0 +1,102 @@
+# Rollback A110 — Playbook
+
+## Quando acionar
+
+- Regressão P1/P2 confirmada pelo A110 imediatamente após merge ou deploy.
+- Queda do SLO primário (latência, erro, throughput) atribuída ao commit recém-mergeado.
+- Indicadores de risco do gate (anomalias de invariantes críticas) sinalizando falha com impacto alto.
+- Relatos de clientes internos ou externos que reproduzem a regressão no build atual.
+- Confirmada indisponibilidade de fluxos essenciais protegidos pelo A110.
+
+## Decidir: Rollback x Forward-fix
+
+| Fator | Questão | Sinal para rollback |
+| --- | --- | --- |
+| Tempo de correção | Fix pode ser entregue em até 30 minutos? | Não |
+| Blast radius | Impacto atual afeta mais de um domínio crítico ou clientes externos? | Sim |
+| Janela operacional | Existe janela de risco (fim de expediente, alto volume, freeze)? | Sim |
+| Dependências | Rollback é autônomo (sem coordenação complexa)? | Sim |
+| Observabilidade | Dados inconclusivos para validar fix forward rapidamente? | Sim |
+
+Checklist:
+
+- [ ] Se a resposta acima preferir rollback (fix > 30 min ou impacto alto), iniciar playbook imediatamente.
+- [ ] Confirmar com o Incident Commander / on-call antes de executar.
+- [ ] Notificar stakeholders críticos sobre a decisão.
+
+## Procedimentos
+
+### 1) GitHub UI (Revert do PR)
+
+1. Abrir o PR mergeado que introduziu a regressão (A110 indica o número no alerta).
+2. Localizar o merge commit na UI (seção "Commits" do PR) e clicar em "Revert".
+3. Confirmar a criação automática de um novo PR com o commit de rollback.
+4. Editar título e descrição para indicar rollback emergencial (ex.: "Rollback: <PR original>").
+5. Solicitar revisão mínima (1 reviewer) e mergear com prioridade máxima.
+6. Acompanhar o deploy automático resultante do merge do PR de rollback.
+
+### 2) Git CLI
+
+1. Identificar o merge:
+   - `git log --merges --oneline` para encontrar o commit.
+   - Ou `gh pr view -w <numero_pr>` para abrir detalhes no navegador.
+2. Executar rollback:
+   - `git checkout main`
+   - `git pull`
+   - `git revert -m 1 <merge_sha>`
+   - Resolver conflitos, se existirem.
+   - `git push origin main`
+3. Para squash merge (commit único), executar `git revert <commit_sha>` sem `-m 1`.
+4. Se preferir PR, criar branch temporária, abrir PR de rollback e seguir política padrão de revisão rápida.
+
+### 3) Release/Deploy
+
+1. Conferir tags disponíveis: `git tag --sort=-creatordate | head`.
+2. Validar release anterior no GitHub: `gh release view <tag-anterior>`.
+3. Reaplicar artefatos, se necessário:
+   - `gh release download <tag-anterior> --dir ./releases/<tag-anterior>`
+   - Verificar integridade dos binários/containers.
+4. Criar release de rollback (quando requerido pelo fluxo):
+   - `gh release create <tag-anterior>-revert --notes "Rollback da release <tag-anterior> devido a regressão detectada pelo A110." --target <commit_revertido>`
+5. Reimplantar a versão anterior conforme o pipeline (ex.: reexecutar workflow de deploy apontando para `<tag-anterior>`).
+6. Confirmar que todos os ambientes afetados receberam a versão revertida.
+
+## Validação pós-rollback
+
+- A110 verde no commit revertido (sem regressões P1/P2).
+- Smoke tests automatizados executados com sucesso (pipeline CI ou scripts manuais).
+- Dashboards de monitoramento (Prometheus/Grafana) mostram métricas normalizadas.
+- Traços Jaeger voltam a chegar com latência esperada.
+- Endpoints de saúde (`/health`, `/ready`) respondem OK.
+- Verificar logs para ausência de novos erros críticos.
+
+## Comunicação
+
+- Comentário no PR de rollback: "Rollback executado às <hora> UTC. Regressão confirmada no A110 (<link alerta>). SLO normalizado."
+- Atualização no CHANGELOG: seção "Fixes" com referência ao rollback e ao PR original.
+- Anúncio interno (Slack/Email):
+  - Impacto observado (serviços/usuários afetados).
+  - Causa preliminar (commit/PR que introduziu regressão).
+  - Mitigação adotada (rollback, versão reinstalada).
+  - Próximos passos (investigação root cause, follow-up tickets).
+- Registrar incidente no sistema de tracking (ex.: PagerDuty/Jira) com status mitigado.
+
+## Prevenção
+
+- Adicionar testes automatizados (golden/property-based) cobrindo o cenário falho.
+- Avaliar ativação de feature flag para limitar rollout futuro.
+- Melhorar alertas do A110 (limiares, notificações redundantes).
+- Revisar hardening do gate (validar invariantes pré-merge, canary automated).
+- Garantir cobertura de seeds e dados críticos nos ambientes de teste.
+- Abrir tickets de follow-up e atribuir responsáveis com prazo.
+
+## Apêndice
+
+- Encontrar merge commit recente: `git log --merges --since="24 hours ago" --oneline`.
+- Listar releases disponíveis: `gh release list --limit 20`.
+- Verificar deploy ativo em Kubernetes: `kubectl rollout status deployment/<servico>`.
+- Conferir histórico de alertas do A110: acessar dashboard `<url_interna>`.
+- Checklist rápido de comandos úteis:
+  - `git revert --continue`
+  - `gh pr create --title "Rollback" --body "Rollback automatizado"`
+  - `kubectl rollout undo deployment/<servico>`
\ No newline at end of file
diff --git a/scripts/a110_add_badge.sh b/scripts/a110_add_badge.sh
new file mode 100644
index 0000000..5a3476c
--- /dev/null
+++ b/scripts/a110_add_badge.sh
@@ -0,0 +1,88 @@
+#!/usr/bin/env bash
+set -euo pipefail
+
+read_remote() {
+  if git rev-parse --is-inside-work-tree >/dev/null 2>&1; then
+    git remote get-url origin 2>/dev/null || return 1
+  else
+    return 1
+  fi
+}
+
+normalize_owner_repo() {
+  local raw="$1"
+  raw="${raw%.git}"
+  case "$raw" in
+    https://github.com/*)
+      printf '%s' "${raw#https://github.com/}"
+      ;;
+    http://github.com/*)
+      printf '%s' "${raw#http://github.com/}"
+      ;;
+    git@github.com:*)
+      printf '%s' "${raw#git@github.com:}"
+      ;;
+    ssh://git@github.com/*)
+      printf '%s' "${raw#ssh://git@github.com/}"
+      ;;
+    git://github.com/*)
+      printf '%s' "${raw#git://github.com/}"
+      ;;
+    *)
+      return 1
+      ;;
+  esac
+}
+
+main() {
+  local remote_url owner_repo badge readme tmp_file
+  remote_url="${GITHUB_REPOSITORY:-}"
+
+  if remote_from_git=$(read_remote); then
+    remote_url="$remote_from_git"
+  fi
+
+  owner_repo=""
+  if [ -n "$remote_url" ]; then
+    if parsed=$(normalize_owner_repo "$remote_url" 2>/dev/null); then
+      owner_repo="$parsed"
+    fi
+  fi
+
+  if [ -z "$owner_repo" ] && [ -n "${GITHUB_REPOSITORY:-}" ]; then
+    owner_repo="$GITHUB_REPOSITORY"
+  fi
+
+  if [ -z "$owner_repo" ]; then
+    exit 0
+  fi
+
+  badge="[![A110 — Invariants](https://github.com/${owner_repo}/actions/workflows/a110-invariants.yml/badge.svg)](https://github.com/${owner_repo}/actions/workflows/a110-invariants.yml)"
+  readme="README.md"
+
+  if [ ! -e "$readme" ]; then
+    : > "$readme"
+  fi
+
+  if grep -Fqx "$badge" "$readme" 2>/dev/null; then
+    exit 0
+  fi
+
+  tmp_file="$(mktemp)"
+  trap 'rm -f "$tmp_file"' EXIT
+
+  {
+    if [ -s "$readme" ]; then
+      printf '%s\n\n' "$badge"
+      cat "$readme"
+    else
+      printf '%s\n' "$badge"
+    fi
+  } > "$tmp_file"
+
+  mv "$tmp_file" "$readme"
+  trap - EXIT
+}
+
+main "$@"
+exit 0
\ No newline at end of file
diff --git a/scripts/a110_gate.py b/scripts/a110_gate.py
new file mode 100644
index 0000000..388495c
--- /dev/null
+++ b/scripts/a110_gate.py
@@ -0,0 +1,386 @@
+#!/usr/bin/env python3
+from __future__ import annotations
+
+import argparse
+import json
+import os
+import re
+import sys
+from dataclasses import dataclass
+from datetime import datetime, timezone
+from pathlib import Path
+from typing import Dict, Iterable, List, Optional, Sequence, Tuple
+import xml.etree.ElementTree as ET
+
+SEVERITY_PATTERN = re.compile(r"\\b\\[?(P1|P2|P3)\\]?\\b", re.IGNORECASE)
+SEED_PATTERNS: Sequence[re.Pattern[str]] = (
+    re.compile(r"seed\\s*=\\s*(\\d{4,})", re.IGNORECASE),
+    re.compile(r"seed[:=]\\s*(\\d{4,})", re.IGNORECASE),
+    re.compile(r"re-?run with (?:--)?seed[=:]?(\\d{4,})", re.IGNORECASE),
+    re.compile(r"minimal failing.*\\bseed\\b[:=]\\s*(\\d{4,})", re.IGNORECASE),
+)
+SEVERITY_ORDER = {"P1": 0, "P2": 1, "P3": 2}
+MAX_JSON_MESSAGE = 512
+MAX_MD_MESSAGE = 120
+
+
+@dataclass
+class FailureItem:
+    suite: str
+    name: str
+    severity: str
+    time: float
+    message: str
+    seed: Optional[str] = None
+
+
+def truncate(value: str, limit: int) -> str:
+    if len(value) <= limit:
+        return value
+    if limit <= 1:
+        return value[:limit]
+    return value[: limit - 1] + "…"
+
+
+def sanitize_line(value: str) -> str:
+    value = value.replace("\r", " ")
+    value = value.replace("\n", " ")
+    value = re.sub(r"\s+", " ", value)
+    return value.strip()
+
+
+def determine_severity(name: str, classname: str) -> str:
+    for target in (name, classname):
+        match = SEVERITY_PATTERN.search(target or "")
+        if match:
+            return match.group(1).upper()
+    return "P2"
+
+
+def parse_time(value: Optional[str]) -> float:
+    if not value:
+        return 0.0
+    try:
+        return float(value)
+    except ValueError:
+        return 0.0
+
+
+def extract_failure_message(node: ET.Element) -> str:
+    if node.text and node.text.strip():
+        return node.text
+    message_attr = node.get("message")
+    if message_attr:
+        return message_attr
+    return ""
+
+
+def build_failure_item(
+    suite: str,
+    testcase: ET.Element,
+    failure_node: ET.Element,
+) -> FailureItem:
+    name = testcase.get("name", "")
+    classname = testcase.get("classname", "")
+    severity = determine_severity(name, classname)
+    time_value = parse_time(testcase.get("time"))
+    raw_message = extract_failure_message(failure_node)
+    if raw_message:
+        first_line = sanitize_line(raw_message.splitlines()[0])
+    else:
+        first_line = sanitize_line(failure_node.get("message", ""))
+    if not first_line:
+        first_line = "No failure message provided."
+    return FailureItem(
+        suite=suite or "",
+        name=name or "",
+        severity=severity,
+        time=time_value,
+        message=first_line,
+    )
+
+
+def parse_junit_report(path: Path) -> Tuple[List[FailureItem], Optional[str]]:
+    try:
+        tree = ET.parse(path)
+    except Exception as exc:  # noqa: BLE001
+        message = f"Failed to parse JUnit report: {exc}"
+        synthetic = FailureItem(
+            suite="JUnitParser",
+            name="report", 
+            severity="P2",
+            time=0.0,
+            message=sanitize_line(message),
+        )
+        return [synthetic], message
+
+    root = tree.getroot()
+    failures: List[FailureItem] = []
+
+    for testsuite in root.iter("testsuite"):
+        suite_name = testsuite.get("name", "")
+        for testcase in testsuite.findall("testcase"):
+            failure_node = testcase.find("failure")
+            error_node = testcase.find("error") if failure_node is None else None
+            node = failure_node or error_node
+            if node is None:
+                continue
+            failures.append(build_failure_item(suite_name, testcase, node))
+
+    return failures, None
+
+
+def read_text_safe(path: Path) -> str:
+    try:
+        return path.read_text(encoding="utf-8")
+    except Exception:  # noqa: BLE001
+        return ""
+
+
+def write_text(path: Path, content: str) -> None:
+    path.parent.mkdir(parents=True, exist_ok=True)
+    path.write_text(content, encoding="utf-8")
+
+
+def collect_seed_matches(lines: Sequence[str]) -> List[Tuple[int, str]]:
+    results: List[Tuple[int, str]] = []
+    for index, line in enumerate(lines):
+        for pattern in SEED_PATTERNS:
+            for match in pattern.finditer(line):
+                seed = match.group(1)
+                results.append((index, seed))
+    return results
+
+
+def map_seeds_to_failures(
+    log_path: Optional[Path],
+    failures: Sequence[FailureItem],
+) -> Dict[str, str]:
+    if not log_path:
+        return {}
+    if not log_path.exists():
+        return {}
+
+    content = read_text_safe(log_path)
+    if not content:
+        return {}
+
+    lines = content.splitlines()
+    matches = collect_seed_matches(lines)
+    if not matches:
+        return {}
+
+    mapping: Dict[str, Tuple[int, str]] = {}
+    for failure in failures:
+        best: Optional[Tuple[int, str]] = None
+        for index, seed in matches:
+            contexts = [lines[index]]
+            if index > 0:
+                contexts.append(lines[index - 1])
+            if index + 1 < len(lines):
+                contexts.append(lines[index + 1])
+            match_found = any(
+                failure.name and failure.name in context or failure.suite and failure.suite in context
+                for context in contexts
+            )
+            if not match_found:
+                continue
+            if best is None or index > best[0]:
+                best = (index, seed)
+        if best is not None:
+            mapping[f"{failure.suite}::{failure.name}"] = best[1]
+    return {key: value for key, value in mapping.items()}
+
+
+def apply_seeds(failures: Iterable[FailureItem], seeds: Dict[str, str]) -> None:
+    for failure in failures:
+        key = f"{failure.suite}::{failure.name}"
+        seed = seeds.get(key)
+        if seed:
+            failure.seed = seed
+
+
+def sort_failures(failures: List[FailureItem]) -> List[FailureItem]:
+    def sort_key(item: FailureItem) -> Tuple[int, str, str]:
+        severity_rank = SEVERITY_ORDER.get(item.severity, SEVERITY_ORDER["P2"])
+        return (severity_rank, item.suite, item.name)
+
+    return sorted(failures, key=sort_key)
+
+
+def compute_counts(failures: Sequence[FailureItem]) -> Dict[str, int]:
+    counts = {"P1": 0, "P2": 0, "P3": 0}
+    for failure in failures:
+        if failure.severity in counts:
+            counts[failure.severity] += 1
+        else:
+            counts["P2"] += 1
+    return counts
+
+
+def determine_gate_status(counts: Dict[str, int]) -> str:
+    if counts.get("P1", 0) > 0 or counts.get("P2", 0) > 0:
+        return "fail"
+    return "pass"
+
+
+def build_json_summary(
+    gate_status: str,
+    counts: Dict[str, int],
+    failures: Sequence[FailureItem],
+    generated_at: str,
+) -> str:
+    meta = {
+        "sha": os.getenv("GITHUB_SHA", ""),
+        "ref": os.getenv("GITHUB_REF", ""),
+        "run_id": os.getenv("GITHUB_RUN_ID", ""),
+        "generated_at": generated_at,
+    }
+    failed_entries: List[Dict[str, object]] = []
+    for item in failures:
+        entry: Dict[str, object] = {
+            "suite": item.suite,
+            "name": item.name,
+            "severity": item.severity,
+            "time": item.time,
+            "message": truncate(item.message, MAX_JSON_MESSAGE),
+        }
+        if item.seed:
+            entry["seed"] = item.seed
+        failed_entries.append(entry)
+
+    payload = {
+        "gate_status": gate_status,
+        "counts": counts,
+        "failed": failed_entries,
+        "meta": meta,
+    }
+    return json.dumps(payload, indent=2, sort_keys=False) + "\n"
+
+
+def escape_md(value: str) -> str:
+    escaped = value.replace("|", "\\|")
+    escaped = escaped.replace("`", "\\`")
+    escaped = escaped.replace("<", "&lt;").replace(">", "&gt;")
+    return escaped
+
+
+def format_time(value: float) -> str:
+    return f"{value:.3f}"
+
+
+def build_failures_table(failures: Sequence[FailureItem]) -> List[str]:
+    lines = ["| Severity | Suite | Test | Time(s) | Seed | Message |", "| --- | --- | --- | --- | --- | --- |"]
+    for item in failures:
+        seed_display = item.seed or ""
+        message_display = truncate(item.message, MAX_MD_MESSAGE)
+        row = "| {severity} | {suite} | {name} | {time} | {seed} | {message} |".format(
+            severity=escape_md(item.severity),
+            suite=escape_md(item.suite),
+            name=escape_md(item.name),
+            time=escape_md(format_time(item.time)),
+            seed=escape_md(seed_display),
+            message=escape_md(message_display),
+        )
+        lines.append(row)
+    return lines
+
+
+def build_repro_section(failures: Sequence[FailureItem]) -> List[str]:
+    lines: List[str] = []
+    for item in failures:
+        if not item.seed:
+            continue
+        command = f"PROPTEST_SEED={item.seed} cargo test -- {item.name}"
+        lines.append(command)
+    return lines
+
+
+def build_markdown_summary(
+    gate_status: str,
+    counts: Dict[str, int],
+    failures: Sequence[FailureItem],
+    generated_at: str,
+) -> str:
+    status_label = gate_status.upper()
+    lines = ["# A110 — Invariants Gate Summary", f"Status: {status_label}"]
+    lines.append("")
+    lines.append("## Counts")
+    lines.append(f"- P1: {counts['P1']}")
+    lines.append(f"- P2: {counts['P2']}")
+    lines.append(f"- P3: {counts['P3']}")
+    lines.append("")
+    rule_desc = (
+        "Gate status is FAIL because P1/P2 failures are blocking." if gate_status == "fail" else "Gate status is PASS because only P3 or no failures were detected."
+    )
+    lines.append(rule_desc)
+    lines.append("")
+
+    if failures:
+        lines.append("## Failing Tests")
+        lines.extend(build_failures_table(failures))
+        lines.append("")
+    else:
+        lines.append("No failing tests detected.")
+        lines.append("")
+
+    repro = build_repro_section(failures)
+    if repro:
+        lines.append("## Como reproduzir")
+        for command in repro:
+            lines.append(f"- `{command}`")
+        lines.append("")
+
+    meta_line = " | ".join(
+        [
+            f"SHA: {os.getenv('GITHUB_SHA', '')}",
+            f"Ref: {os.getenv('GITHUB_REF', '')}",
+            f"Run ID: {os.getenv('GITHUB_RUN_ID', '')}",
+            f"Generated at: {generated_at}",
+        ]
+    )
+    lines.append("---")
+    lines.append(meta_line)
+    lines.append("")
+    return "\n".join(lines)
+
+
+def parse_args(argv: Optional[Sequence[str]] = None) -> argparse.Namespace:
+    parser = argparse.ArgumentParser(description="Generate A110 invariants gate summary.")
+    parser.add_argument("--junit", required=True, help="Path to the JUnit XML report.")
+    parser.add_argument("--log", help="Path to the cargo test log for seed extraction.")
+    parser.add_argument("--summary-json", required=True, help="Path to write the JSON summary.")
+    parser.add_argument("--summary-md", required=True, help="Path to write the Markdown summary.")
+    return parser.parse_args(argv)
+
+
+def main(argv: Optional[Sequence[str]] = None) -> None:
+    args = parse_args(argv)
+    junit_path = Path(args.junit)
+    log_path = Path(args.log) if args.log else None
+    json_path = Path(args.summary_json)
+    md_path = Path(args.summary_md)
+
+    failures, parse_error = parse_junit_report(junit_path)
+    if parse_error:
+        print(f"[a110_gate] {parse_error}", file=sys.stderr)
+
+    seeds = map_seeds_to_failures(log_path, failures)
+    apply_seeds(failures, seeds)
+
+    ordered_failures = sort_failures(list(failures))
+    counts = compute_counts(ordered_failures)
+    gate_status = determine_gate_status(counts)
+    generated_at = datetime.now(timezone.utc).isoformat()
+
+    json_summary = build_json_summary(gate_status, counts, ordered_failures, generated_at)
+    markdown_summary = build_markdown_summary(gate_status, counts, ordered_failures, generated_at)
+
+    write_text(json_path, json_summary)
+    write_text(md_path, markdown_summary)
+
+    sys.exit(0)
+
+
+if __name__ == "__main__":
+    main()
diff --git a/scripts/a110_run_invariants.sh b/scripts/a110_run_invariants.sh
new file mode 100644
index 0000000..4003a5f
--- /dev/null
+++ b/scripts/a110_run_invariants.sh
@@ -0,0 +1,306 @@
+#!/usr/bin/env bash
+set -euo pipefail
+IFS=$'\n\t'
+umask 022
+
+START_TIME="$(date -u '+%s')"
+EXIT_NEXTEST=111
+EXIT_CARGO=111
+SIGNAL_NAME=""
+FINALIZED=0
+UNEXPECTED_FAILURE=0
+PYTHON_BIN=""
+
+ARTIFACT_DIR="artifacts"
+
+timestamp() {
+  date -u '+%Y-%m-%dT%H:%M:%SZ'
+}
+
+log_info() {
+  printf '[info] %s %s\n' "$(timestamp)" "$*"
+}
+
+log_warn() {
+  printf '[warn] %s %s\n' "$(timestamp)" "$*" >&2
+}
+
+log_error() {
+  printf '[error] %s %s\n' "$(timestamp)" "$*" >&2
+}
+
+ensure_artifacts() {
+  if ! mkdir -p "${ARTIFACT_DIR}"; then
+    log_error 'Unable to create artifacts directory.'
+    UNEXPECTED_FAILURE=1
+    return 1
+  fi
+
+  local targets=(
+    "${ARTIFACT_DIR}/junit.xml"
+    "${ARTIFACT_DIR}/cargo-test.log"
+    "${ARTIFACT_DIR}/exitcodes.env"
+    "${ARTIFACT_DIR}/summary.md"
+    "${ARTIFACT_DIR}/summary.json"
+  )
+  local t
+  for t in "${targets[@]}"; do
+    rm -f "$t" 2>/dev/null || true
+  done
+
+  return 0
+}
+
+set_proptest_defaults() {
+  local defaults=(
+    'PROPTEST_CASES:256'
+    'PROPTEST_MAX_SHRINK_TIME:2000'
+    'PROPTEST_MAX_SHRINK_ITERS:1024'
+    'PROPTEST_MAX_GLOBAL_REJECTS:4096'
+    'PROPTEST_MAX_LOCAL_REJECTS:1024'
+    'PROPTEST_MAX_FLAT_MAP_REJECTS:4096'
+  )
+  local entry name value
+  for entry in "${defaults[@]}"; do
+    name="${entry%%:*}"
+    value="${entry##*:}"
+    if [ -z "${!name-}" ]; then
+      printf -v "$name" '%s' "$value"
+    fi
+    export "$name"
+  done
+}
+
+resolve_python() {
+  if [ -n "${PYTHON_BIN}" ] && command -v "${PYTHON_BIN}" >/dev/null 2>&1; then
+    return 0
+  fi
+  local candidate
+  for candidate in python3 python; do
+    if command -v "$candidate" >/dev/null 2>&1; then
+      PYTHON_BIN="$candidate"
+      return 0
+    fi
+  done
+  log_error 'Unable to locate a python interpreter (python3 or python).'
+  UNEXPECTED_FAILURE=1
+  return 1
+}
+
+write_exitcodes() {
+  if ! printf 'EXIT_NEXTEST=%s\nEXIT_CARGO=%s\n' "$EXIT_NEXTEST" "$EXIT_CARGO" > "${ARTIFACT_DIR}/exitcodes.env"; then
+    log_error 'Failed to write artifacts/exitcodes.env.'
+    UNEXPECTED_FAILURE=1
+  fi
+}
+
+emit_summary() {
+  local end_time duration gate_should_fail p1_failures p2_failures p3_failures now
+
+  end_time="$(date -u '+%s')"
+  duration=$(( end_time - START_TIME ))
+  if [ "$duration" -lt 0 ]; then
+    duration=0
+  fi
+
+  p1_failures=0
+  p2_failures=0
+  p3_failures=0
+
+  if [ "$EXIT_NEXTEST" -ne 0 ]; then
+    p1_failures=$(( p1_failures + 1 ))
+  fi
+  if [ "$EXIT_CARGO" -ne 0 ]; then
+    p2_failures=$(( p2_failures + 1 ))
+  fi
+
+  gate_should_fail='false'
+  if [ "$p1_failures" -gt 0 ] || [ "$p2_failures" -gt 0 ]; then
+    gate_should_fail='true'
+  fi
+
+  now="$(timestamp)"
+
+  if ! {
+    printf '# Invariant Run Summary\n\n'
+    printf -- '- Timestamp (UTC): %s\n' "$now"
+    printf -- '- Duration: %ss\n' "$duration"
+    printf -- '- Signal: %s\n' "${SIGNAL_NAME:-none}"
+    printf -- '- [P1] cargo nextest run --all --all-features --no-fail-fast --profile ci: exit %s\n' "$EXIT_NEXTEST"
+    printf -- '- [P2] cargo test --all --all-features -- --nocapture: exit %s\n' "$EXIT_CARGO"
+    printf -- '- Gate should fail: %s\n' "$gate_should_fail"
+    printf '\nArtifacts:\n'
+    printf -- '- junit: %s/junit.xml\n' "$ARTIFACT_DIR"
+    printf -- '- cargo log: %s/cargo-test.log\n' "$ARTIFACT_DIR"
+    printf -- '- exit codes: %s/exitcodes.env\n' "$ARTIFACT_DIR"
+  } > "${ARTIFACT_DIR}/summary.md"; then
+    log_error 'Failed to write artifacts/summary.md.'
+    UNEXPECTED_FAILURE=1
+  fi
+
+  if ! {
+    printf '{\n'
+    printf '  "timestamp_utc": "%s",\n' "$now"
+    printf '  "duration_seconds": %s,\n' "$duration"
+    printf '  "signal": "%s",\n' "${SIGNAL_NAME:-none}"
+    printf '  "commands": [\n'
+    printf '    {"name": "[P1] cargo nextest run", "exit_code": %s, "artifact": "%s/junit.xml"},\n' "$EXIT_NEXTEST" "$ARTIFACT_DIR"
+    printf '    {"name": "[P2] cargo test", "exit_code": %s, "artifact": "%s/cargo-test.log"}\n' "$EXIT_CARGO" "$ARTIFACT_DIR"
+    printf '  ],\n'
+    printf '  "failures": {"p1": %s, "p2": %s, "p3": %s},\n' "$p1_failures" "$p2_failures" "$p3_failures"
+    printf '  "gate_should_fail": %s\n' "$gate_should_fail"
+    printf '}\n'
+  } > "${ARTIFACT_DIR}/summary.json"; then
+    log_error 'Failed to write artifacts/summary.json.'
+    UNEXPECTED_FAILURE=1
+  fi
+
+  log_info "Total duration: ${duration}s"
+  if [ "$gate_should_fail" = 'true' ]; then
+    log_warn 'Gate would fail based on recorded exit codes.'
+  else
+    log_info 'Gate would pass based on recorded exit codes.'
+  fi
+}
+
+finalize() {
+  if [ "$FINALIZED" -eq 1 ]; then
+    return
+  fi
+  FINALIZED=1
+  write_exitcodes
+  emit_summary
+  if [ "$UNEXPECTED_FAILURE" -ne 0 ]; then
+    log_warn 'Unexpected issues encountered; check artifacts for details.'
+  fi
+}
+
+handle_exit() { finalize; }
+
+handle_signal() {
+  SIGNAL_NAME="$1"
+  log_warn "Caught signal: $SIGNAL_NAME"
+  finalize
+  exit 0
+}
+
+run_with_tee() {
+  local log_path="$1"
+  shift
+  if ! resolve_python; then
+    return 100
+  fi
+  "$PYTHON_BIN" - "$log_path" "$@" <<'PY'
+import subprocess
+import sys
+from typing import List
+
+def main() -> int:
+    if len(sys.argv) < 3:
+        sys.stderr.write('Usage error: expected log path and command.\n')
+        return 97
+    log_path = sys.argv[1]
+    cmd: List[str] = sys.argv[2:]
+    try:
+        f = open(log_path, 'w', encoding='utf-8')
+    except OSError as exc:
+        sys.stderr.write(f'Unable to open log file {log_path}: {exc}\n')
+        return 98
+    try:
+        p = subprocess.Popen(
+            cmd,
+            stdout=subprocess.PIPE,
+            stderr=subprocess.STDOUT,
+            text=True,
+            encoding='utf-8',
+            errors='replace'
+        )
+    except OSError as exc:
+        f.close()
+        sys.stderr.write(f'Failed to start command: {exc}\n')
+        return 99
+    assert p.stdout is not None
+    try:
+        for chunk in p.stdout:
+            sys.stdout.write(chunk)
+            sys.stdout.flush()
+            f.write(chunk)
+        p.stdout.close()
+        rc = p.wait()
+    finally:
+        f.flush()
+        f.close()
+    return rc
+
+if __name__ == '__main__':
+    sys.exit(main())
+PY
+}
+
+trap 'handle_signal INT'  INT
+trap 'handle_signal TERM' TERM
+trap 'handle_exit'        EXIT
+
+log_info 'Preparing environment for invariant run.'
+if ! ensure_artifacts; then
+  EXIT_NEXTEST=111
+  EXIT_CARGO=111
+  finalize
+  exit 0
+fi
+
+set_proptest_defaults
+
+log_info 'Running cargo nextest invariants.'
+if cargo nextest run --all --all-features --no-fail-fast --profile ci; then
+  EXIT_NEXTEST=0
+  log_info 'cargo nextest run completed successfully.'
+else
+  EXIT_NEXTEST=$?
+  log_error "cargo nextest run failed with exit code ${EXIT_NEXTEST}."
+fi
+
+# Copia JUnit do perfil 'ci' (ou variantes), com fallback para stub.
+JUNIT_DEST="${ARTIFACT_DIR}/junit.xml"
+JUNIT_CANDIDATES=(
+  "target/nextest/ci/junit.xml"
+  "target/nextest/default/junit.xml"
+)
+FOUND_JUNIT=""
+for candidate in "${JUNIT_CANDIDATES[@]}"; do
+  if [ -f "$candidate" ]; then
+    FOUND_JUNIT="$candidate"
+    break
+  fi
+done
+if [ -z "${FOUND_JUNIT}" ]; then
+  set +e
+  FOUND_JUNIT="$(ls target/nextest/*/junit.xml 2>/dev/null | head -n1)"
+  set -e
+fi
+if [ -n "${FOUND_JUNIT}" ] && [ -f "${FOUND_JUNIT}" ]; then
+  cp -f "${FOUND_JUNIT}" "${JUNIT_DEST}"
+else
+  cat > "${JUNIT_DEST}" <<'XML'
+<?xml version="1.0" encoding="UTF-8"?>
+<testsuite name="cargo-nextest" tests="1" failures="1">
+  <testcase classname="cargo-nextest" name="missing-junit">
+    <failure message="P1: JUnit report not found">
+      cargo nextest did not produce a junit.xml artifact under target/nextest/*/junit.xml
+    </failure>
+  </testcase>
+</testsuite>
+XML
+fi
+
+log_info 'Running cargo test (full suite).'
+if run_with_tee "${ARTIFACT_DIR}/cargo-test.log" cargo test --all --all-features -- --nocapture; then
+  EXIT_CARGO=0
+  log_info 'cargo test completed successfully.'
+else
+  EXIT_CARGO=$?
+  log_error "cargo test failed with exit code ${EXIT_CARGO}."
+fi
+
+finalize
+exit 0
-- 
2.51.0

